{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_PULILtKAgku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_pages(topic):\n",
        "  link = 'https://wuzzuf.net/search/jobs/?a=hpb%7Cspbg&q=' + topic.replace(' ', '%20')\n",
        "  response = requests.get(link)\n",
        "  soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "  while not soup.find('strong'):\n",
        "    response = requests.get(link)\n",
        "    soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "  num_of_jobs = int(soup.find('strong').text)\n",
        "  num_of_pages = num_of_jobs // 15 + num_of_jobs % 15\n",
        "\n",
        "  titles_lst, links_lst, occupations_lst, companies_lst, locations_lst, specs_lst = [], [], [], [], [], []\n",
        "\n",
        "  for pageNum in range(num_of_pages):\n",
        "    page_link = link + '&start=' + str(pageNum)\n",
        "    page = requests.get(page_link)\n",
        "    soup = BeautifulSoup(page.content, 'lxml')\n",
        "\n",
        "    titles = soup.find_all(\"h2\", 'css-m604qf')\n",
        "    titles_lst += [title.a.text for title in titles]\n",
        "    links_lst += [title.a['href'] for title in titles]\n",
        "\n",
        "    occupations_types = ['On-site', 'Hybrid', 'Remote', 'Full Time', 'Internship', 'Part Time', 'Shift based', 'Freelance / Project']\n",
        "    occupations = soup.find_all('div', 'css-1lh32fc')\n",
        "    occupations_lst += [' | '.join([t for t in occupations_types if t in oc.text]) for oc in occupations]\n",
        "\n",
        "    companies = soup.find_all('div', 'css-d7j1kk')\n",
        "    companies_lst += [name.a.text.rstrip(' -') for name in companies]\n",
        "    locations_lst += [location.span.text.strip() for location in companies]\n",
        "\n",
        "    specs = soup.find_all(\"div\", {'class': 'css-y4udm8'})\n",
        "    specs_lst += [''.join(div.text.strip() for div in spec.find_all('div')[1:]) for spec in specs]\n",
        "\n",
        "  scraped_data = {}\n",
        "  scraped_data['Title'] = titles_lst\n",
        "  scraped_data['Link'] = links_lst\n",
        "  scraped_data['Occupation'] = occupations_lst\n",
        "  scraped_data['Company'] = companies_lst\n",
        "  scraped_data['Location'] = locations_lst\n",
        "  scraped_data['Specs'] = specs_lst\n",
        "\n",
        "  df = pd.DataFrame(scraped_data)\n",
        "  return scraped_data, df"
      ],
      "metadata": {
        "id": "jDKsoPNln46i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_dfs(dfs):\n",
        "  df = pd.concat(dfs)\n",
        "  df = df.drop_duplicates()\n",
        "  return df\n",
        "\n",
        "def combine_dicts(dicts):\n",
        "  combined_dict = {}\n",
        "  for key in dicts[0].keys():\n",
        "    combined_dict[key] = []\n",
        "    for dict in dicts:\n",
        "      combined_dict[key] += dict[key]\n",
        "  return combined_dict"
      ],
      "metadata": {
        "id": "XRJO9_TQCY6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_dict, ds_df = scrape_pages('Data science')\n",
        "ml_dict, ml_df = scrape_pages('Machine learning')\n",
        "ai_dict, ai_df = scrape_pages('Artificial intelligence')\n",
        "\n",
        "combined_dict = combine_dicts([ds_dict, ml_dict, ai_dict])\n",
        "combined_df = combine_dfs([ds_df, ml_df, ai_df])\n",
        "\n",
        "combined_df.to_csv('data.csv', index=False)"
      ],
      "metadata": {
        "id": "3fr7FUIron1A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}